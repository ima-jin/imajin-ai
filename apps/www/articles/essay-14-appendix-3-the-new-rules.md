---
title: "Appendix 3: The New Rules"
subtitle: "AI regulated by human trust, not institutional authority"
description: "The substrate must be regulated. Not by bureaucracy, not by legislation written by people who can't explain what a token is. By the trust graph. The institutions with the most destructive capacity and the least accountability are the last ones who should wield it unsupervised."
date: "2026-XX-XX"
author: "Ryan Veteze"
---

## The Precedent

The licensed electrician isn't licensed because a government said so.

They're licensed because other electricians vouched for their competence and staked their reputation on it. The guild existed before the regulation. The credential came after the trust network — not before.

Master electricians trained apprentices. Apprentices proved themselves over years. The community of practitioners decided who was ready to wire a building with people inside it. When the government eventually formalized it, they were encoding something that already existed: a web of human trust built on demonstrated competence and personal accountability.

Every regulated profession followed the same path. Doctors. Engineers. Pilots. Lawyers. The trust network came first. The institutional framework came second. The institution didn't create the trust — it inherited it.

We need to do the same thing with AI. And we need to do it before the institutions get there first and do it wrong.

---

## The Argument

If the LLM is an industrial substrate — and it is — then it requires the same professional shielding that every other industrial substrate eventually got.

But here's the part the industry doesn't want to hear: **regulation by human trust means some institutions are structurally disqualified.**

The licensed electrician has a name. A reputation. A career that ends if they wire a building that burns down. Their name is on the permit. The chain of accountability is complete — from the person who trained them, through the work they've done, to the building you're standing in.

That's the .fair chain applied to professional practice. Attribution. Provenance. Accountability. Every decision traceable to the human who made it.

Now look at the institutions with the most compute and the most destructive capacity.

---

## Why Governments and Militaries Can't Be Trusted with the Substrate

Not because they're evil. Because their structure is incompatible with accountability.

**Classification exists specifically to break attribution.** The entire purpose of security clearance is to ensure that decisions cannot be traced by the public to the humans who made them. That's the opposite of .fair. That's the opposite of the trust graph. That's the deliberate, structural collapse of the attribution chain.

**No individual accountability.** A drone strike authorized through seven layers of command has no single name attached. A surveillance system deployed across a population has no operator whose reputation is at stake. The institution absorbs the liability — which means nobody absorbs it.

**No reputation at stake.** An AI professional in the trust graph has their name in the chain. Bad output traces back to them. Their position in the network — their ability to earn, to be queried, to be trusted — depends on the quality of their judgment. A military AI deployment has none of this. The humans involved are shielded by the institution, not by competence.

**Secrecy is the mechanism.** Institutional trust scales through secrecy. Human trust scales through the graph. These are incompatible architectures. You cannot have an accountable AI practice inside a system designed to prevent accountability.

This isn't a political argument. It's a structural one. The same way you can't wire a house safely without grounding, you can't deploy AI safely without attribution. Institutions that are structurally incapable of attribution are structurally incapable of safe deployment.

---

## The New Rules

**1. AI becomes a regulated profession.**

Not regulated by governments. Regulated by practitioners. The trust graph of AI professionals — people who understand the substrate, who have demonstrated competence, whose names are in the .fair chain of everything they've produced.

Your credential is your position in the network. Your reputation is your stake. You're accountable because your career exists inside a web of human relationships that will hold you to your own standards.

**2. Governments and militaries are barred from unsupervised AI use.**

Any government or military deployment of AI requires a licensed professional operator — a human in the trust graph whose name is on the output. Not a department. Not a classification level. A person, with a reputation, in the chain.

No classified AI. If the attribution chain can't be public, the deployment can't happen. Secrecy and accountability are mutually exclusive. Choose one.

**3. Goodbye nukes.**

This is where it goes. If you follow the substrate argument honestly — if you accept that industrial substrates require professional shielding, that the shielding is human accountability, and that institutions designed around secrecy are structurally incapable of accountability — then AI-assisted weapons systems are disqualified by architecture, not by treaty.

You don't ban them with a document. You make them impossible to deploy within a trust-regulated framework. No licensed professional will put their name on a nuclear targeting system. No trust graph will carry it. No .fair chain will attribute it.

The weapon doesn't disappear because someone signed an agreement. It disappears because no accountable human will touch it.

**4. The guild enforces itself.**

The trust graph is self-regulating. Bad actors lose position. Professionals who enable unaccountable deployments lose their network. The economics of the trust graph make ethical practice more rewarding than unethical practice — not through morality, but through architecture.

You don't need a regulatory body. You need a trust graph where reputation is economic and accountability is structural. The professionals who give a damn about what happens when people use the substrate — they're the regulation.

---

## The Objection

"This is naive. Governments will use AI regardless of what a trust graph says."

Yes. The same way governments used electricity before it was regulated. The same way militaries used chemical weapons before the Geneva Convention. The same way corporations dumped waste before environmental regulation.

The pattern is always the same: the powerful use the substrate first, without shielding, and the damage eventually creates the political will for constraint.

The difference this time: the trust graph doesn't need political will. It doesn't need a treaty. It doesn't need a vote. It needs practitioners who refuse to participate in unaccountable deployments. And it needs an economic infrastructure where that refusal is sustainable — where the professional who says "I won't put my name on that" can still earn, still practice, still thrive.

That infrastructure exists now.

The constraint isn't a law. The constraint is a network of humans who decided that accountability is non-negotiable. And an economic system that makes that decision rational.

---

## If We Regulate Who Can Wire a House

We regulate who can wire a civilization.

The electrician's license isn't bureaucracy. It's the accumulated wisdom of every building that burned down before someone said: the person who does this work has to know what they're doing, and has to put their name on it.

AI needs the same thing. Not from a government that can't define the substrate. From the humans who work with it every day and know exactly what it can do — and what it can do to you.

The trust graph is the license. The .fair chain is the permit. The network is the guild.

The new rules aren't written by legislators. They're written by practitioners, in code, in protocol, in the architecture of a system that makes accountability the default and secrecy architecturally impossible.

That's the regulation. Not a law. A network.

---

*This is Appendix 3 of Essay 14: How to Use AI Properly.*
*The appendices are living documents. This one is a starting position, not a final one.*
*If you think this is naive, I'd love to hear your alternative that includes accountability.*
*ryan@imajin.ai*
